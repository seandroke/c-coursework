Sean DrokeCIS 3207March 15, 2019Project 2Simple Unix Shell	The Unix shell is the most integral part of the Unix environment. It provides the I/O stream necessary to communicate with the operating system and directory structure as a whole. The shell must be accurate, as without accuracy, communication with the operating system cannot happen effectively. Designing a shell requires progressive thinking. To effectively implement a solution, every use case of the program must be determined in early development as serious changes are difficult to implement later. Constructing the shell component by component I found was the best solution, but integrating pieces tended to cause difficulties and required extensive testing to verify function.	First, a method needed to be found to delineate between batch file invocation and displaying a standard user prompt. To accomplish this, the main function would take in potential arguments and be able to verify that such an argument was a file that can be opened. Each line was then processed as a command. In a similar way, if no batch file was present the prompt would appear and take in user input. A challenge existed in dynamically allocating the memory for both of these input methods. It is important when reading input that the user does not easily run out of memory, especially with large batch files. After completing this, I conducted a unit test on the main function and prompt function to verify it was collecting user and batch file strings.	With the program now capable of accepting strings it was then necessary to process the input stream. Converting the arrays of characters to a uniform string command was important in organizing the data to be distributed to functions based on needs. Using built in string functions, it was easy to determine whether or not they contained special characters that would affect their processing needs. Depending on symbols, the commands would need either background processing, I/O redirection, pipes, or they would exhibit normal behavior. Implementing this processor allowed for each command to be “blocked” and sent to its respective function. To test this processor, I passed the program input from batch file and from user input and wrote a test case within each of the functions called by the processor. The functions would simply print what the processer passed to them. It was at this point that integration became a challenge as the program was growing in moving pieces and required many areas of error handling.	Once the commands were being sent effectively to the necessary processor functions it was time to utilize some built in Unix/Linux functions to accomplish some of the tasks. Calling to a parser, I tokenized each string command into arguments separated by white space, doing so was necessary for execvp to run. For normal processing, I learned a traditional procedure of fork, execvp, and finally wait to completion. This same strategy was used partially in the other functions as well. The only difference between normal and background was the need for a wait condition. However, execvp alone could not accomplish every task necessary. So, I created a library of possible string commands within checker and in cases where these commands were possible I would run the strings through the function to output results. This made error handling easy to test as along the way I could input error codes to represent when certain conditions were failing such as instances when a bad character was passed with a command. Once the checker was working I began to finish I/O processing and piping.	To process I/O I had to traverse the command several times to achieve some preliminary variables. I retrieved a count of the number of inputs and outputs. From that I could break down possible scenarios. In each scenario, the necessary file descriptors contained open files from arguments each passed as a parameter of dup and an attempt would be made to pass it to checker or execvp. Subsequently the I/O would be redirected according to the file argument. Integrating this took some time as understanding each possible scenario and how to verify the scenario was occurring required extensive code that had to be made understandable.	Finally, I implemented piping. Pipe would take a pair of commands split with the pipe symbol and process one, allowing the information from that process to be referenced by the next. Doing this involved organizing the command into its respective arguments. Traversing until the pipe character and traversing after the pipe character and appending arguments and commands to function variables was the process by which this was accomplished. Once variables were set the file descriptors were initialized, the pipe occurred, and pid’s generated. If pid was 0 the first portion would execute, otherwise the ladder portion would execute creating another fork from the previous pid. At completion, both sides of the pipe executed and the command was complete. Testing this involved passing in some different possible combinations of input through main and monitoring how the function responded. Once this was integrated, the shell was complete.	Overall, the shell is a very complex program for Unix based operating systems. It is the main tool for accomplishing nearly anything without a GUI. Briefly analyzing the backend code required to make this happen was interesting and certainly something overlooked with routine use.